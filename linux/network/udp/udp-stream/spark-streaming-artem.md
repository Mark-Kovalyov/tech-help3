# Возможности Spark Streaming для аналитики данных в потоковом режиме

* Артем Гогин https://www.youtube.com/watch?v=uir6D0bEdZE


## Пакетная обработка

* Consistent
* Controlled
* Group by/Order by

## Чтобы анализировать чаще возможно:

* Запускать джоб несколько раз в день
* Делать новый streaming job

### Запускать джоб несколько раз в день

* Plus:
  * логика уже проверена и работает
  * легко управлять и вносить обновления
* Minus:
  * Мелкие partitions (часовые)
  * Джойны будут перечитываться целиком
  * Чтобы считать дневные агрегаты нужно читать все дневные партиции или переписывать логику



## Замена батча на стриминг

* Запускается 1 раз и работает пока не упадет
* Все время следит за новыми данными
* После падение продолжаем где закончили
* Нужен источник данных в которых идут Append
  * New files
    * HDSF
    * S3
    * local
  * Queues
    * Kafka
* Группировки в пределах ???

# Формализуем параметры стриминга

* Input/Output
* Хранение результатов потока (коммиты и суммы)
* Частота деления данных (раз в сек/min/hour)
* Latency
* Логика обработки
  * Stateless
  * Statefull

## 1 Input/Output

* Input
  * Kafka
  * Files
  * Custom connectors

* Output
  * Kafka
  * Files
  * JDBC

## Хранение результатов потока (коммиты и суммы)

Можем хранить
* Прочитанные входные данные

## 3 Частота чтения данных

* Микробатчи от 1 сек (можно и быстрее)
* Нужно успеть выполнить всю логику
* Если не успеваем образуется очередь
* Возможно ограничить количество сообщений в микробатче

## 4 Latency

## 5.1 Логика обработки данных

### 5.1 Stateless

* Filter
* Map
* Custom logic for an executor
* joins with static dataset
  * With RAM
  * With files
  * With SQL
  * With NoSQL

### 5.2 Statefull

* Input: Append кликов
* Output: Витрина кликов

* Aggregates
  * With window
  * With watermark - retention
* Joins with another stream (not recommended!)

## Типичные задачи

* Онлайн загрузка данных в хранилище
* Онлайн обогащение
* Онлайн статистика по клиентам
* Онлайн мониторинг безопасности
* Онлайн ML

## Онлайн загрузка

* output
  * by partitions
  * into parquet/orc

Periodical compaction

## Обогащение

* Обогащаем большим или маленьким датасетом
* Динамическим или стат

* Маленький датасет помещается в память
* Большой датасет в NoSQL

* 2 стриминговых датасета джойним по такой-же логике держа второй в памяти или NoSQL

## Безопасность (мониторинг)

* Ищем подозрительные операции
* Маленькое окно в 1 час
* Фильтруем
* Отправляем подозрительные события в MQ

## Онлайн отчетность (вариант 1) statefull

* Суммы внутри окна (24 час)
* Обрабатываем в памяти 2 дня для late events
* Раз в минуту переписываем выходные партиции

## Онлайн отчетность (вариант 1) stateless

* Обработка без окна
* Суммы храним в NoSQL
* Для каждого клика делаем +1

## Онлайн применение ML

.....

## Выводы по Spark Streaming

Plus:
* SQL/Python/Scla/Java API for batch and Streaming
* distributed operations
* разнообразные коннекторы
* консистентное чтение и управление коммитами
* эффективные планы обработки данных
* поддержка ML
* exactly one гарантии с идемпотентным приемником

Minus
* не очень богатый функционал управления окнами
* стабильность зависит от инфраструктуры где работает Spark
