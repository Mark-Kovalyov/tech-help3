# Возможности Spark Streaming для аналитики данных в потоковом режиме 

* Артем Гогин https://www.youtube.com/watch?v=uir6D0bEdZE

## Замена батча на стриминг

* Запускается 1 раз и работает пока не упадет
* Все время следит за новыми данными
* После падение продолжаем где закончили
* Нужен источник данных в которых идут Append
  * New files
    * HDSF
    * S3
    * local
  * Queues
    * Kafka
* Группировки в пределах ???

## 3 Частота чтения данных

* Микробатчи от 1 сек (можно и быстрее)
* Нужно успеть выполнить всю логику
* Если не успеваем образуется очередь
* Возможно ограничить количество сообщений в микробатче

## 4 Latency

## 5.1 Логика обработки данных

### 5.1 Stateless

* Filter
* Map
* Custom logic for an executor
* joins with static dataset
  * With RAM
  * With files
  * With SQL
  * With NoSQL

### 5.2 Statefull

* Input: Append кликов
* Output: Витрина кликов

* Aggregates
  * With window
  * With watermark - retention
* Joins with another stream (not recommended!)

## Типичные задачи

* Онлайн загрузка данных в хранилище
* Онлайн обогащение
* Онлайн статистика по клиентам
* Онлайн мониторинг безопасности
* Онлайн ML

## Онлайн загрузка

* output
  * by partitions
  * into parquet/orc

Periodical compaction

## Обогащение

* Обогащаем большим или маленьким датасетом
* Динамическим или стат

* Маленький датасет помещается в память
* Большой датасет в NoSQL

* 2 стриминговых датасета джойним по такой-же логике держа второй в памяти или NoSQL

## Безопасность (мониторинг)

* Ищем подозрительные операции
* Маленькое окно в 1 час
* Фильтруем
* Отправляем подозрительные события в MQ

